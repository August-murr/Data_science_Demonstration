{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SQMK7R-vmQUi",
        "MQId08b28YS2",
        "XMni2RDq9MHy",
        "l45AOWIlBM4o",
        "gRvbWG6UCPCA",
        "r_bnmygeDVL-",
        "RnLXm4aQ73t0",
        "oV6aqjie9hdu",
        "87EUbaJW93z8",
        "GeA8Jwwg_8Ft",
        "3-bj3j2OBYSq",
        "BdQLCPYVDaYj"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/August-murr/Data_science_Demonstration/blob/main/Fine-Tuning%20Machine%20Translation%20Model/fine_tuning_machine_translation_model_with_peft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        " In this notebook, we'll leverage the [Parallel Movie Subtitles](https://www.kaggle.com/datasets/augustmurr/movie-parallel-dataset) dataset, to effectively fine-tune a machine translation model in Google Colab. This process will be facilitated by the GPU resources provided by Colab and utilizing the Hugging Face libraries. Specifically, we'll employ Hugging Face's PEFT (Parameter Efficient Fine-Tuning) library with a LoRA adapter. This approach allows us to train a small number of parameters, making it computationally efficient. Moreover, it helps mitigate the risk of catastrophic forgetting, contributing to enhanced performance in translating movie subtitles."
      ],
      "metadata": {
        "id": "ZD038UiD5yvM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries"
      ],
      "metadata": {
        "id": "SQMK7R-vmQUi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdX99J-YluEv"
      },
      "outputs": [],
      "source": [
        "!pip install transformers peft accelerate sentencepiece datasets evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**transformers**: Library for accessing pre-trained NLP models and tools.\n",
        "\n",
        "**peft**: parameter efficent fine-tuning library for NLP models.\n",
        "\n",
        "**accelerate**: for faster training through distributed computing.\n",
        "\n",
        "**sentencepiece**: Used by Hugging Faces Tokenizers\n",
        "\n",
        "**datasets**: Library for easy access and manipulation of commonly used datasets\n",
        "\n",
        "**evaluate**: Library for evaluating the performance of NLP models."
      ],
      "metadata": {
        "id": "XCQvX_vXmYX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dataset from Kaggle"
      ],
      "metadata": {
        "id": "MQId08b28YS2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell uses your Kaggle API token to get data. Make sure you have your token ready or create a new one before running the cell."
      ],
      "metadata": {
        "id": "9EAG4eaX8e1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary library for uploading Kaggle API token\n",
        "from google.colab import files\n",
        "\n",
        "# Uploading the Kaggle API token file\n",
        "files.upload()\n",
        "# Removing existing Kaggle directory if it exists\n",
        "!rm -r ~/.kaggle\n",
        "# Creating a new Kaggle directory\n",
        "!mkdir ~/.kaggle\n",
        "# Moving the uploaded Kaggle API token to the Kaggle directory\n",
        "!mv ./kaggle.json ~/.kaggle/\n",
        "# Setting appropriate permissions for the Kaggle API token file\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "GYtsi4_hnJP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset from Kaggle using the Kaggle API\n",
        "!kaggle datasets download -d augustmurr/movie-parallel-dataset"
      ],
      "metadata": {
        "id": "rrrBizNBo-6o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa08624-d0e5-49fd-a088-fd0b0478db0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading movie-parallel-dataset.zip to /content\n",
            " 98% 135M/138M [00:04<00:00, 41.0MB/s]\n",
            "100% 138M/138M [00:04<00:00, 34.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the downloaded dataset\n",
        "#change the path if yours is different\n",
        "!unzip \"/content/movie-parallel-dataset.zip\""
      ],
      "metadata": {
        "id": "ND79m5wo8OL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Data Loading and Cleaning\n",
        "\n",
        "The dataset is organized as CSV files with two columns (the first one representing English). To prepare it for training, we need to do some cleaning. There are three functions explained below.\n",
        "\n",
        "1. The first function, `find_files_with_name`, locates the paths of specific files within folders. Since there are two types of CSV files (time-based and line-by-line data), we load them separately. For this notebook, we'll focus on using time-based subtitles, which include parallel subtitles for each minute of a movie.\n",
        "\n",
        "2. The `concatenate_and_clean_csv_files` function removes unnecessary characters such as \"[],(),♪♪,\" which often represent additional explanations from the movie and do not contribute to the translation. It then combines the data to form a comprehensive dataset. Proper removal of these characters is crucial as their presence can significantly impact the model's performance and lead to hallucinations, where the model may start translating sentences that do not actually exist in the dataset.\n",
        "\n",
        "3. The `split_dataframe` function divides the data into training, validation, and test sets."
      ],
      "metadata": {
        "id": "XMni2RDq9MHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "SaBXz2s49SS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_files_with_name(file_path, file_name):\n",
        "    matched_files = []\n",
        "    # Walk through all subfolders\n",
        "    for root, dirs, files in os.walk(file_path):\n",
        "        if file_name in files:\n",
        "            # Construct the full file path and add it to the list\n",
        "            matched_files.append(os.path.join(root, file_name))\n",
        "    return matched_files"
      ],
      "metadata": {
        "id": "1bUIX09H-9Xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def concatenate_and_clean_csv_files(file_paths):\n",
        "    # Initialize an empty DataFrame to hold all the data\n",
        "    large_df = pd.DataFrame()\n",
        "\n",
        "    # Loop through all file paths and concatenate them into large_df\n",
        "    for file_path in file_paths:\n",
        "        df = pd.read_csv(file_path, skiprows=1, names=['language_1', 'language_2']).dropna()\n",
        "\n",
        "        # Clean the data using regex\n",
        "        pattern = r'\\(.*?\\)|\\[.*?\\]|♪♪.*?♪♪|\\w+:|♪'\n",
        "        df.loc[:, 'language_1'] = df['language_1'].apply(lambda x: re.sub(pattern, '', x).strip())\n",
        "        df.loc[:, 'language_2'] = df['language_2'].apply(lambda x: re.sub(pattern, '', x).strip())\n",
        "\n",
        "        # Append the cleaned dataframe to large_df\n",
        "        large_df = pd.concat([large_df, df], ignore_index=True)\n",
        "\n",
        "    # Rename the columns to 'input' and 'target'\n",
        "    large_df.rename(columns={'language_1': 'input', 'language_2': 'target'}, inplace=True)\n",
        "\n",
        "    return large_df"
      ],
      "metadata": {
        "id": "n2h0raOx_Ben"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataframe(df, train_size=0.7, val_size=0.15, test_size=0.15):\n",
        "    assert train_size + val_size + test_size == 1, \"The split sizes must sum up to 1\"\n",
        "\n",
        "    # Calculate the proportion of validation set relative to the combined validation and test set size\n",
        "    val_relative_size = val_size / (val_size + test_size)\n",
        "\n",
        "    # Split the data into training and remaining data\n",
        "    train_df, remaining_df = train_test_split(df, train_size=train_size, random_state=42, shuffle=False)\n",
        "\n",
        "    # Split the remaining data into validation and test sets\n",
        "    val_df, test_df = train_test_split(remaining_df, train_size=val_relative_size, random_state=42, shuffle=False)\n",
        "\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "NCd-wgkq_DFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"There are parallel datasets available for four language pairs: English to Thai, English to French, English to Arabic, and English to Indonesian. You can modify the path below to select the language pair you want to fine-tune the data for. In this case, we chose English to Thai. Additionally, you have the option to choose between line-by-line data or time-based data.\""
      ],
      "metadata": {
        "id": "KwohZm_5H9Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paths = find_files_with_name(\"/content/english to thai\",\"parallel_subtitle_time_based.csv\")\n",
        "data = concatenate_and_clean_csv_files(paths)"
      ],
      "metadata": {
        "id": "6uU7bIpf_Yze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Here, you can select the proportion of data you want for training, validation, and testing. We have chosen the default configuration, which is `train_size=0.7`, `val_size=0.15`, and `test_size=0.15`.\""
      ],
      "metadata": {
        "id": "Uc6BoQNLIqKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train,val,test = split_dataframe(data)"
      ],
      "metadata": {
        "id": "hkde31Ws_Wcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## loading the model and tokenizer\n",
        "To make training more efficient, we'll change all the weights from 32-bit floats to 16-bit floats."
      ],
      "metadata": {
        "id": "l45AOWIlBM4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainer\n",
        "from evaluate import load\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "TtZXKpRfCJFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "model.eval()  # Set the model to eval mode\n",
        "if torch.cuda.is_available():\n",
        "    model = model.to('cuda').half()  # Move model to CUDA for FP16 operations"
      ],
      "metadata": {
        "id": "RmZSSfUMCMrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Translation Function\n",
        "\n",
        "This function takes a list of sentences and a language ID, then tokenizes and translates them using the model. Make sure to pick the correct language IDs from this [link](https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt)."
      ],
      "metadata": {
        "id": "gRvbWG6UCPCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate a list of sentences to a specified language\n",
        "def translate_sentences(model, tokenizer, sentences,target_lang_id,src_language_id=\"en_XX\"):\n",
        "    translated_sentences = []\n",
        "    for sentence in sentences:\n",
        "        tokenizer.src_lang = src_language_id\n",
        "        encoded = tokenizer(sentence, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "\n",
        "        # Move tensors to same device as model\n",
        "        if torch.cuda.is_available():\n",
        "            encoded = encoded.to('cuda')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = model.generate(\n",
        "                **encoded,\n",
        "                forced_bos_token_id=tokenizer.lang_code_to_id[target_lang_id]\n",
        "            )\n",
        "        translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "        translated_sentences.append(translation[0])\n",
        "    return translated_sentences"
      ],
      "metadata": {
        "id": "XtOfoeM9C-zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Functions\n",
        "\n",
        "For evaluation, we'll employ two methods. First, the BLEU score, and then a sentence similarity model. While BLEU is quicker and simpler, it doesn't consider alternative translations with similar meanings. Therefore, a sentence similarity model provides a more accurate evaluation of overall performance.\n",
        "\n",
        "here's the [link](https://huggingface.co/setu4993/LEALLA-small) to the sentence similarities hugging face page."
      ],
      "metadata": {
        "id": "r_bnmygeDVL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate BLEU score given two lists of strings\n",
        "def evaluate_bleu(predictions, references):\n",
        "    bleu_metric = load('bleu')\n",
        "    formatted_references = [[sentence] for sentence in references]\n",
        "    bleu_result = bleu_metric.compute(predictions=predictions, references=formatted_references)\n",
        "    return bleu_result"
      ],
      "metadata": {
        "id": "Oy9qRbRgEL4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizerFast\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vivKXpS6ESXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "similarity_tokenizer = BertTokenizerFast.from_pretrained(\"setu4993/LEALLA-small\")\n",
        "similarity_model = BertModel.from_pretrained(\"setu4993/LEALLA-small\")\n",
        "similarity_model.eval()"
      ],
      "metadata": {
        "id": "1E4cqQN7ETxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the average similarity between pairs of sentences in two lists\n",
        "def calculate_average_pairwise_similarity(list_sentences1, list_sentences2):\n",
        "    assert len(list_sentences1) == len(list_sentences2), \"The lists must be of the same length\"\n",
        "\n",
        "    similarity_scores = []\n",
        "\n",
        "    for sentence1, sentence2 in zip(list_sentences1, list_sentences2):\n",
        "        # Encode sentences\n",
        "        inputs = similarity_tokenizer([sentence1, sentence2], return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Generate embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = similarity_model(**inputs)\n",
        "\n",
        "        # Get the pooler_output for sentence embeddings\n",
        "        embeddings = outputs.pooler_output\n",
        "\n",
        "        # Compute L2 normalized embeddings\n",
        "        normalized_embeddings = F.normalize(embeddings, p=2, dim=1)\n",
        "\n",
        "        # Compute similarity score for the pair\n",
        "        similarity_score = torch.matmul(\n",
        "            normalized_embeddings[0].unsqueeze(0),\n",
        "            normalized_embeddings[1].unsqueeze(0).transpose(0, 1)\n",
        "        )\n",
        "\n",
        "        similarity_scores.append(similarity_score.item())\n",
        "\n",
        "    # Calculate the average similarity score\n",
        "    average_similarity = np.mean(similarity_scores)\n",
        "\n",
        "    return average_similarity"
      ],
      "metadata": {
        "id": "i9-HSmY9FggX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Original Model with BLEU and Sentence Similarity\n",
        "\n",
        "Next, we'll assess the model using BLEU and a similarity function. This helps us make comparisons with the fine-tuned model later on."
      ],
      "metadata": {
        "id": "RnLXm4aQ73t0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcjJGpLz_VOx"
      },
      "outputs": [],
      "source": [
        "english_sentences = list(train['input'][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYbF61X4Do6d"
      },
      "outputs": [],
      "source": [
        "thai_sentences = list(train['target'][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mvsn33AEclO"
      },
      "outputs": [],
      "source": [
        "thai_translations = translate_sentences(model,tokenizer,english_sentences,\"th_TH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BMYW2IjFAXl"
      },
      "outputs": [],
      "source": [
        "calculate_average_pairwise_similarity(thai_translations,thai_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enn9U_ihf11U"
      },
      "outputs": [],
      "source": [
        "evaluate_bleu(thai_translations,thai_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning with PEFT\n",
        "\n",
        "To evaluate the model, we switched the weights to 16-bit floats. Now, for fine-tuning, we'll use 16-bit floats as well. However, the Hugging Face trainer requires the model to be in 32-bit floats because it handles the conversion internally. So, we'll reload the model accordingly.\n",
        "\n",
        "If you encounter memory issues, you can use the following code to free up GPU memory:\n",
        "```python\n",
        "# Clear up the GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "```"
      ],
      "metadata": {
        "id": "oV6aqjie9hdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "xZB7mkY19nBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
        "model_for_peft = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "RI3a5sxg9qDI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA Configuration\n",
        "\n",
        "This is where we set up the LoRA adapter's parameters, like the number of weights to train, dropout rate, and which layers to include. To find out the layer names of our model, we ran the following code that prints the structure of the transformer model:\n",
        "\n",
        "```python\n",
        "# Print all named modules in the model to identify layer names\n",
        "for name, module in model.named_modules():\n",
        "    print(name, module.__class__.__name__)\n",
        "```\n",
        "\n",
        "Understanding these layer names helps us configure the LoRA adapter for fine-tuning."
      ],
      "metadata": {
        "id": "87EUbaJW93z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import get_peft_model\n",
        "from peft import LoraConfig, TaskType"
      ],
      "metadata": {
        "id": "Xv0fVH9n9xtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=64,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"]\n",
        ")"
      ],
      "metadata": {
        "id": "S8haY-0j-8Za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model = get_peft_model(model_for_peft, peft_config)"
      ],
      "metadata": {
        "id": "OGKcHM4t-9lA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll utilize the function below to determine the total number of parameters in the model and specify how many we are going to train."
      ],
      "metadata": {
        "id": "3xLhej8m_G3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_trainable_model_parameters(model):\n",
        "    trainable_model_params = 0\n",
        "    all_model_params = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_model_params += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_model_params += param.numel()\n",
        "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
      ],
      "metadata": {
        "id": "GmYm6c7d_CKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_number_of_trainable_model_parameters(peft_model)"
      ],
      "metadata": {
        "id": "EkJ2OfTE_hBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizing the Datasets"
      ],
      "metadata": {
        "id": "GeA8Jwwg_8Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the dataset and create a DataLoader if necessary\n",
        "from transformers import MBart50TokenizerFast\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "Bcz5X5Af_vqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MBart50TokenizerFast.from_pretrained(model_checkpoint,src_lang=\"en_XX\", tgt_lang=\"th_TH\")"
      ],
      "metadata": {
        "id": "vv91K7om_1Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1q8D3yLhsAq"
      },
      "outputs": [],
      "source": [
        "train.reset_index(drop=True, inplace=True)\n",
        "train_dataset = Dataset.from_pandas(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-rK_KI7BNo7"
      },
      "outputs": [],
      "source": [
        "train.reset_index(drop=True, inplace=True)\n",
        "train_dataset = Dataset.from_pandas(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yz4dux7HBNpM"
      },
      "outputs": [],
      "source": [
        "val.reset_index(drop=True, inplace=True)\n",
        "val_dataset = Dataset.from_pandas(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "movPf6vjBNpM"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(examples):\n",
        "    model_inputs = tokenizer(\n",
        "        text=examples[\"input\"],\n",
        "        text_target=examples[\"target\"],\n",
        "        max_length=256,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2FcfGf5BNpN"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(tokenize_function,batched=True)\n",
        "train_dataset = train_dataset.remove_columns(['input', 'target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQqmsMEWBNpN"
      },
      "outputs": [],
      "source": [
        "val_dataset = val_dataset.map(tokenize_function,batched=True)\n",
        "val_dataset = val_dataset.remove_columns(['input', 'target'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Arguments\n",
        "\n",
        "In this section, we set up training parameters such as learning rate, batch size, 16-bit float training, the number of epochs, and specify a directory to save the training checkpoints."
      ],
      "metadata": {
        "id": "3-bj3j2OBYSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75GTABw1BNpO"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2SIu19tk7BT"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=peft_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiIPaVOvq7xn"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UvlBs34q4EY"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/peft checkpoints\",#change to your prefered dir\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=4,  # Adjust as needed\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=50,  # Tune as per requirement\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_steps=500,  # Adjust as needed\n",
        "    fp16=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "tP9s4pcDCL03"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrcRF2tRk6-O"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "# Save the model to the specified directory\n",
        "model_path = \"/content/drive/MyDrive/peft models/en_to_thai model\"\n",
        "peft_model.save_pretrained(model_path)\n",
        "# Save the tokenizer to the same directory as the model\n",
        "tokenizer_path = \"/content/drive/MyDrive/peft models/en_to_thai thai\"\n",
        "tokenizer.save_pretrained(tokenizer_path)"
      ],
      "metadata": {
        "id": "BGqF21xVCZ3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the Fine-Tuned Model\n",
        "\n",
        "Now, we'll load the model and the tokenizer from the saved path and evaluate them in the same manner as we did with the original model."
      ],
      "metadata": {
        "id": "BdQLCPYVDaYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "cLfaQ3uDDXto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "44fLowjl7Ndu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model and tokenizer\n",
        "fine_tuned_model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/en to thai fine tuned model\")\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/en to thai fine tuned model\")"
      ],
      "metadata": {
        "id": "ZGxUGwo5FZH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to the correct device (e.g., GPU or CPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "fine_tuned_model.eval()  # Set the model to eval mode\n",
        "fine_tuned_model.to(device)\n",
        "fine_tuned_model.half()"
      ],
      "metadata": {
        "id": "pIIpJnj4Fdsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpFdhgctFwda"
      },
      "outputs": [],
      "source": [
        "english_sentences = list(train['input'][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqe5NwfeFwdb"
      },
      "outputs": [],
      "source": [
        "thai_sentences = list(train['target'][:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn5W128kFwdc"
      },
      "outputs": [],
      "source": [
        "thai_translations = translate_sentences(fine_tuned_model,fine_tuned_tokenizer,english_sentences,\"th_TH\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zE46-yn8Fwdc"
      },
      "outputs": [],
      "source": [
        "calculate_average_pairwise_similarity(thai_translations,thai_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pznW2JFVFwdc"
      },
      "outputs": [],
      "source": [
        "evaluate_bleu(thai_translations,thai_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tested both models on a sample of the data, and here are the results:\n",
        "\n",
        "- Similarity Score: 0.45 ➔ 0.55\n",
        "- BLEU Score: 0.02 ➔ 0.09\n",
        "\n",
        "The similarity score considers the overall meaning and indicates that the model adapted well to translating movie dialogues. On the other hand, the BLEU score, which represents the structure of the translation, showed a more significant improvement. The model learned to translate longer pieces of text better."
      ],
      "metadata": {
        "id": "WOLlMJ73tBxE"
      }
    }
  ]
}